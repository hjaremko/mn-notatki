\documentclass[../mn-notatki.tex]{subfiles}

\begin{document}

\section{Układy równań liniowych}

\begin{tcolorbox}
\[
Ax = b
\]
\[
\text{gdzie } A \in \mathbb{R}^{n \times n}, b \in \mathbb{R}^n
\]
\end{tcolorbox}

\subsection{Rozkład LU}

\begin{tcolorbox}
\[
A = LU
\]
\[
Ax = L(Ux) = b
\]
\[
\begin{cases}
Lz = b\\
Ux = z
\end{cases}
\]
\end{tcolorbox}

\subsubsection{Metoda Doolittle'a}
\subsubsection{Metoda Crouta}
\subsubsection{Metoda Cholesky'ego}
\subsubsection{Istnienie rozkładu LU}

\subsection{Algorytm eliminacji Gaussa}

Oznaczamy przez $A_i$ $i$-ty wiersz macierzy $A$ \textit{(jako wektor)}.
Wtedy $k$-ty krok algorytmu Gaussa możemy podzielić na następujące etapy:
\begin{itemize}
    \item W podmacierzy $A(k\ldots n, k\ldots n)$ wybierz element głowny $x_k$.
    \item Zamień wiersze lub kolumny tak, aby element głowny $x_k$ znalazł się
    na pozycji $a_kk$.
    \item Wyzeruj elementy pod elementem głownym.
    \[
    A_i \leftarrow A_i - \frac{a_{ik}}{a_{kk}} \cdot A_k, \text{~~ dla} k+1, \ldots, n
    \]
\end{itemize}

\subsubsection{Wybór elementu głównego}

\begin{tcolorbox}
\textbf{Częściowy} - wybieramy największy na moduł element w pierwszej kolumnie
bieżącej podmacierzy.
\end{tcolorbox}

\begin{tcolorbox}
\textbf{Pełny} - wybieramy największy na moduł element w całej
bieżącej podmacierzy.
\end{tcolorbox}

\begin{tcolorbox}
\textbf{Skalowany} - na poczaku całego algorytmu dla każdego wiersza obliczamy
jego normę $s_i$ \textit{(najczęściej maksimum)}, w $k$-tym kroku wybieramy
element $a_{ik}$ z pierwszej kolumny podmacierzy, którego
$\frac{|a_{ik}|}{s_i}$ jest największe.
\end{tcolorbox}

\subsection{Odwracanie macierzy trójkątnych}
\subsection{Odwracanie przez rozkład LU}

\begin{tcolorbox}
Jeśli mamy rozkład $A = LU$ to macierz odwrotną do $A$ możemy wyznaczyć przez
\begin{itemize}
    \item $A^{-1} = U^{-1} \cdot L^{-1}$
\end{itemize}
Powyższy algorytm wykorzystuje fakt, że
\[
(D\cdot G)^{-1} = G^{-1} \cdot D^{-1}
\]
\end{tcolorbox}

Usprawniamy algorytm rozwiązując równania
\[
Ax_i = LUx_i = e_i
\]
gdzie $e_i$ to kolejne wektory jednostkowe. Wektory $x_i$ są kolejnymi
kolumnamy macierzy $A^{-1}$.

\subsection{Poprawianie iteracyjne}

\begin{itemize}
    \item $x_0$ - przybliżenie początkowe
    \item $A = LU$ - macierz
    \item $\varepsilon$ - zakładana precyzja
\end{itemize}
\begin{tcolorbox}
\textbf{Algorytm - jeden krok}
\begin{itemize}
    \item $r_k = b - Ax_k$
    \item jeżeli $|r_k| < \varepsilon$ zakończ i zwróć $x_k$
    \item $e_k = $ rozwiązanie układu $Ae_k = LUe_k = r_k$
    \item $x_{k+1} = x_k + e_k$
\end{itemize}
\end{tcolorbox}

\begin{itemize}
    \item Wektor rezydualny $r_k$ należy obliczyć w wyższej precyzji
    \textit{(np. \texttt{double} dla \texttt{float}, \texttt{long double} dla
    \texttt{double})} bo mamy tam znaczne kasowanie bitów znaczących.
    \item Jeżeli obliczając $x_0$ dokonaliśmy rozkładu $LU$ macierzy $A$ to
    jeden krok poprawiania jest tani \textit{(koszt kwadratowy)}.
\end{itemize}


\pagebreak
\end{document}
