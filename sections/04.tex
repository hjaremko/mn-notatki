\documentclass[../mn-notatki.tex]{subfiles}

\begin{document}

\section{Układy równań liniowych}

\begin{tcolorbox}
\[
Ax = b
\]
\[
\text{gdzie } A \in \mathbb{R}^{n \times n}, b \in \mathbb{R}^n
\]
\[
cond(A) = ||A^{-1}||\cdot ||A||
\]
\end{tcolorbox}

\subsection{Rozkład LU}

\begin{tcolorbox}
\[
A = LU
\]
\[
Ax = L(Ux) = b \Rightarrow \begin{cases}
Lz = b\\
Ux = z
\end{cases}
\]
\end{tcolorbox}

\[
\begin{bmatrix}
a_{11} & a_{12} & \ldots & a_{1n}\\
a_{21} & a_{22} & \ldots & a_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{n1} & a_{n2} & \ldots & a_{nn}
\end{bmatrix}
=
\begin{bmatrix}
l_{11} & 0 & \ldots & 0\\
l_{21} & l_{22} & \ldots & 0\\
\vdots & \vdots & \ddots & \vdots\\
l_{n1} & l_{n2} & \ldots & l_{nn}
\end{bmatrix}
\begin{bmatrix}
u_{11} & u_{12} & \ldots & u_{1n}\\
0      & u_{22} & \ldots & u_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
0      & 0      & \ldots & u_{nn}
\end{bmatrix}
\]
\[
\begin{bmatrix}
a_{11} & a_{12} & \ldots & a_{1n}\\
a_{21} & a_{22} & \ldots & a_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{n1} & a_{n2} & \ldots & a_{nn}
\end{bmatrix}
=
\begin{bmatrix}
l_{11}u_{11} & l_{11}u_{12} & \ldots & l_{11}u_{1n}\\
l_{21}u_{11} & l_{21}u_{12} + l_{22}u_{22} & \ldots & l_{21}u_{1n} + l_{22}u_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
l_{n1}u_{11} & l_{n1}u_{12} + l_{n2}u_{22} & \ldots & \sum_{i=1}^{n} l_{ni}u_{in}
\end{bmatrix}
\]

\textbf{Rozkład Crouta:} $u_{ii} = 1$\\
\textbf{Rozkład Doolittle'a:} $l_{ii} = 1$\\
\textbf{Rozkład Cholesky'ego:} $l_{ii} = u_{ii}$

% \pagebreak

\subsubsection{Metoda Gaussa}
Zerując $i$-tym wierszem $j$-ty wiersz należy wpisać na pozycji $l_{ji}$
wspólczynnik przez który został pomnożony wiersz $i$-ty.

\subsubsection{Metoda Doolittle'a}
% Dobierając odpowiednią kolejność rozwiązywania równań
% \begin{gather*}
% u_{11} = a_{11} \tab \ldots \tab u_{1n} = a_{1n}\\
% l_{21} = \frac{a_{21}}{u_{11}} \tab \ldots \tab l_{n1} = \frac{a_{n1}}{u_{11}}\\
% u_{22} = a_{22} - l_{21}u_{12}\tab\ldots\tab u_{2n} = a_{2n} - l_{21}u_{1n}\\
% l_{32} = \frac{a_{32} - l_{31}u_{12}}{u_{22}}\tab\ldots\tab l_{n2} = \frac{a_{n2} - l_{n1}u_{12}}{u_{22}}\\
% \end{gather*}
\[
u_{ij} = a_{ij} - \sum_{k=1}^{i-1} l_{ik} u_{kj}, \tab \text{dla ~~} j = i, i+1, \ldots, n
\]
\[
l_{ji} = \frac{ a_{ji} - \sum_{k=1}^{i-1} l_{jk} u_{ki} }{u_{ii}}, \tab \text{dla ~~} j = i+1, i+2, \ldots, n
\]

Z ostatniego równania wynika, że metoda nie zadziała, gdy
$u_{ii}=0$.

\subsubsection{Metoda Crouta}

Metoda ta jest analogiczną do metody Doolittle’a, różnica polega na tym, że
diagonala macierzy $U$ jest wypełniona liczbami $1$.

\subsubsection{Metoda Cholesky'ego}

\begin{tcolorbox}
Jeśli macierz $A$ jest \textbf{symetryczna i dodatnio określona} to istnieje
macierz dolnotrójkątna $L$ taka, że $L\cdot L^T = A$.\\
Ten rozkład nazywamy rozkładem Cholesky'ego.\\

\textbf{NIE SPRAWDZAMY} czy macierz jest dodatnio określona. Wykonujemy
algorytm i sprawdzamy czy pod pierwiastiem nie otrzymamy liczby ujemnej.\\
\textbf{Należy} sprawdzić czy macierz jest symetryczna.
\end{tcolorbox}
\begin{gather*}
l_{11} = \sqrt{a_{11}}\\
l_{j1} = \frac{a_{j1}}{l_{11}}\\
l_{ii} = \sqrt{a_{ii} - \sum_{k=1}^{i-1} l_{ik}^2}, \text{~~ dla ~~} j = 2, 3, \ldots, n\\
l_{ji} = \frac{1}{l_{ii}} \left( a_{ji} - \sum_{k=1}^{i-1} l_{jk} l_{ik} \right),
\text{~~ dla ~~}
\begin{aligned}
&i = 2,3 \ldots, n\\
&j = i +1, i+2, \ldots, n
\end{aligned}
\end{gather*}

\subsubsection{Istnienie rozkładu LU}

\begin{tcolorbox}
Jeśli $A = [a_{ij}]$ jest macierzą kwadratową $n\times n$ taką, że wszystkie
minory główne są nieosobliwe, tzn.:
\[
\begin{vmatrix}
a_{11} & \ldots & a_{1k}\\
\vdots & \ddots & \vdots\\
a_{k1} & \ldots & a_{kk}
\end{vmatrix} \neq 0, \text{~~~~~dla~~~} 1,\ldots,n
\]
to istnieją rozkłady $A = LU$ Crouta i Doolittle'a.
\end{tcolorbox}
\begin{tcolorbox}
Jeśli macierz $A \in \mathbb{R}^{n\times n}$ jest nieosobliwa to istnieją
rozkłady $PA = LU$ Crouta i Doolittle'a, gdzie $P$ jest macierzą permutacji.
\end{tcolorbox}

\subsection{Algorytm eliminacji Gaussa}

Oznaczamy przez $A_i$ $i$-ty wiersz macierzy $A$ \textit{(jako wektor)}.
Wtedy $k$-ty krok algorytmu Gaussa możemy podzielić na następujące etapy:
\begin{itemize}
    \item W podmacierzy $A(k\ldots n, k\ldots n)$ wybierz element głowny $x_k$.
    \item Zamień wiersze lub kolumny tak, aby element głowny $x_k$ znalazł się
    na pozycji $a_kk$.
    \item Wyzeruj elementy pod elementem głownym.
    \[
    A_i \leftarrow A_i - \frac{a_{ik}}{a_{kk}} \cdot A_k, \text{~~ dla ~~} k+1, \ldots, n
    \]
\end{itemize}

\subsubsection{Wybór elementu głównego}

\begin{tcolorbox}
\textbf{Częściowy} - wybieramy największy na moduł element w pierwszej kolumnie
bieżącej podmacierzy.
\end{tcolorbox}

\begin{tcolorbox}
\textbf{Pełny} - wybieramy największy na moduł element w całej
bieżącej podmacierzy.
\end{tcolorbox}

\begin{tcolorbox}
\textbf{Skalowany} - na początku całego algorytmu dla każdego wiersza obliczamy
jego normę $s_i$ \textit{(najczęściej maksimum)}, w $k$-tym kroku wybieramy
element $a_{ik}$ z pierwszej kolumny podmacierzy, którego
$\frac{|a_{ik}|}{s_i}$ jest największe.
\end{tcolorbox}

\subsection{Odwracanie macierzy trójkątnych}

\begin{align*}
&l_{ii}' = \frac{1}{l_{ii}}\\
&l_{ij}' = -l_{ii}' \sum_{k=j}^{i-1} l_{ik} l_{kj}' \tab \begin{aligned}
&i = 1, 2, \ldots, n\\
&j = 1, 2, \ldots, i -1
\end{aligned}
\end{align*}
\begin{align*}
&u_{ii}' = \frac{1}{u_{ii}}\\
&u_{ij}' = -u_{ii}' \sum_{k=j}^{i-1} u_{ki} l_{jk}' \tab \begin{aligned}
&j = 1, 2, \ldots, i -1\\
&i = 1, 2, \ldots, n
\end{aligned}
\end{align*}

\subsection{Odwracanie przez rozkład LU}

\begin{tcolorbox}
Jeśli mamy rozkład $A = LU$ to macierz odwrotną do $A$ możemy wyznaczyć przez
\begin{itemize}
    \item $A^{-1} = U^{-1} \cdot L^{-1}$
\end{itemize}
Powyższy algorytm wykorzystuje fakt, że
\[
(D\cdot G)^{-1} = G^{-1} \cdot D^{-1}
\]
\end{tcolorbox}

Usprawniamy algorytm rozwiązując równania
\[
Ax_i = LUx_i = e_i
\]
gdzie $e_i$ to kolejne wektory jednostkowe. Wektory $x_i$ są kolejnymi
kolumnamy macierzy $A^{-1}$.

\subsection{Poprawianie iteracyjne}

% \begin{itemize}
    % \item $x_0$ - przybliżenie początkowe
    % \item $A = LU$ - macierz
    % \item $\varepsilon$ - zakładana precyzja
% \end{itemize}
\begin{tcolorbox}
\textbf{Algorytm - jeden krok}
\begin{itemize}
    \item $r_k = b - Ax_k$
    \item jeżeli $|r_k| < \varepsilon$ zakończ i zwróć $x_k$
    \item $e_k = $ rozwiązanie układu $Ae_k = LUe_k = r_k$
    \item $x_{k+1} = x_k + e_k$
\end{itemize}
\end{tcolorbox}

\begin{itemize}
    \item Wektor rezydualny $r_k$ należy obliczyć w wyższej precyzji
    \textit{(np. \texttt{double} dla \texttt{float}, \texttt{long double} dla
    \texttt{double})} bo mamy tam znaczne kasowanie bitów znaczących.
    \item Jeżeli obliczając $x_0$ dokonaliśmy rozkładu $LU$ macierzy $A$ to
    jeden krok poprawiania jest tani \textit{(koszt kwadratowy)}.
\end{itemize}


\pagebreak
\end{document}
